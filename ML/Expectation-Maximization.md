# EM算法(Expectation-Maximization)
## Background
EM（Expectation-Maximum）算法也称期望最大化算法，是最常见的隐变量估计方法。常被用来学习高斯混合模型（Gaussian mixture model，简称GMM）的参数；隐马尔科夫算法（HMM）、LDA主题模型的变分推断等等。  
EM算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，其中一个为期望步（E步），另一个为极大步（M步），所以算法被称为EM算法。EM算法受到缺失思想影响，最初是为了解决数据缺失情况下的参数估计问题。  
其基本思想是：首先根据己经给出的观测数据，估计出模型参数的值；然后再依据上一步估计出的参数值估计缺失数据的值，再根据估计出的缺失数据加上之前己经观测到的数据重新再对参数值进行估计，然后反复迭代，直至最后收敛，迭代结束。

## Motivation
在使用极大似然估计计算目标分布时，其假设是模型的结构已知。但假如模型的结构信息不全，例如目标分布是一个分布族而不是单一的分布时，极大似然估计难以解决问题。  
在上述假设中，决定样本属于哪一个具体分布需要模型的参数。但是利用极大似然估计计算模型参数又需要先确定分布的结构，因此就成了循环论证。  
EM的思想就是先初始化一组参数，用来确定分布的结构，称为Expectation阶段。然后在Maximization阶段中则对参数进行更新，供下次迭代使用，直到收敛。  
其中可以进行更新的参数称为模型的参数，而决定模型结构的信息则称为潜变量。

## Pipeline
Input：观察数据$x = (x^1, x^2, x^3, \cdot, x^m)$，联合分布$p(x,z|\theta)$，条件分布$p(z|x,\theta)$, 迭代次数$J$  
1. 随机初始化模型参数$\theta$的初值$\theta^0$
2. $for\:\:\: j \:\:\:  from\:\:\:  1 \:\:\:  to \:\:\:  J$  
    E步：计算联合分布的条件概率期望：
    $$Q_i(z^i):=P(z^i|x^i, \theta)$$   
    M步：极大化$L(\theta)$ ,得到$\theta$: 
    $$\theta:= arg \max_\theta \sum_{i=1}^m \sum_{z^i}Q_i(z^i) \log P(x^i, z^i|\theta)$$  

重复E、M步骤直到$\theta$收敛
输出：模型参数$\theta$

## [Proof](https://zhuanlan.zhihu.com/p/36331115)