# Bayesian inference

## Background
​在机器学习领域，通常将监督学习 (supervised learning) 划分为两大类方法：生成模型 (generative model) 与判别模型 (discriminative model)，贝叶斯方法正是生成模型的代表。
$$p(A|B) = \frac {P(B|A)P(A)}{P(B)}$$

## Methods
假设特征向量和标签的联合概率分布为$P(X,Y)$，则训练数据$T$从该分布中iid产生。  
朴素贝叶斯需要学习的是条件概率分布$P(Y|X)$。
$$P(Y|X) = \frac {P(X|Y)P(Y)}{P(X)}$$
其中：  
1. $P(Y)$是先验分布，可以通过训练集估计（极大似然）。  
令$D_c$表示训练集$D$中的第$c$类样本组成的集合，若有充足的独立同分布样本，则可容易地估计出类别的先验概率：
$$P(c) = \frac {|D_c|}{|D|}$$
2. $P(X=x|Y=c_k)$是条件概率分布。当feature的维度过高时，直接根据样本出现的频率来估计将会遇到严重的困难，很多样本取值在训练集中根本没有出现。  
为了避开上述障碍，朴素贝叶斯法 (naive Bayes) 采用了 “**属性条件独立性假设**”，对已知的类别，假设所有属性相互独立，换言之，假设每个属性独立地对分类结果发生影响。由此，条件概率可以重写为：
$$P(x|c) = \prod_{i=1}^d P(x_i|c)$$  
对离散属性而言，零$D_{c,x_i}$表示$D_c$在第$i$个属性上取值为$x_i$的样本组成的集合，则条件概率可估计为：
$$P(x_i|c) = \frac {|D_{c,x_i}|}{|D_c|}$$
对于连续属性，假定$p(x_i|c)~N(\mu_{c,i},\sigma^2_{c,i})$，其中$\mu_{c,i}$和$\sigma^2_{c,i}$分别是第c类样本在第i个属性上取值的均值和方差，则：
$$P(x_i|c)  = \frac 1{\sqrt{2\pi}\sigma_{c,i}} \exp\left(-\frac{{(x_i-\mu_{c,i})}^2}{2\sigma^2_{c,i}}\right)$$
回到贝叶斯定理，我们的目标是给定一个x ，推断其后验概率分布$P(c|x)$ ，即该条数据属于每个类的概率是多少，然后选择概率最大的类别作为x的类输出。那么将贝叶斯定理结合属性条件独立性假设，可以得到：
$$P(c|x) = \frac {P(x|c)P(c)}{P(x)} = \frac {P(c)}{P(x)} \prod_{i=1}^d P(x_i|c)$$
由于对所有的类别c,$P(x)$相同，因此判定准则为：
$$\hat y  = arg \max_{c \in Y}P(c|x) = arg \max_{c \in Y} P(c) \prod_{i=1}^d P(x_i|c)$$

## 拉普拉斯平滑
在直接使用极大似然估计法时，需要注意，若某个属性值在训练集中没有与某个类同时出现，则直接基于之前的公式进行概率估计，再进行判别将出现问题。  
为了避免其他属性携带的信息被训练集中未出现的属性值 “抹去”，在估计概率值时通常需要进行 “平滑” (smoothing)，我们常用 “拉普拉斯修正” (Laplacian correction)。  
具体来说，令N表示训练集D中可能的类别数 ， $N_i$表示第$i$个属性可能的取值数，则类别先验概率和条件概率分别修正为：
$$\hat P(c) = \frac{|D_c|+1}{|D|+N}$$
$$\hat P(x_i|c) = \frac {|D_{c,x_i}|+1}{|D_c|+N}$$
而此时，对于任何输入数据，推测的概率值永远会大于 0。显然，拉普拉斯修正避免了因训练集不充分而导致概率估值为零的问题，并且在训练集变大时，修正过程所引入的先验 (prior) 的影响也会逐渐变得可忽略，使得估计值逐渐趋向于实际概率值。